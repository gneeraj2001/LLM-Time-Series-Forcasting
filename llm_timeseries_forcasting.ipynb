{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Methodology***\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "The model architecture adapts large language models (LLMs) for time series forecasting by treating sequential ticket volumes like a language modeling problem : enabling the use of transformer attention to model temporal dependencies.\n",
        "\n",
        "---\n",
        "\n",
        "### Sequence Input as Embeddings\n",
        "\n",
        "Daily ticket volumes and engineered features are converted into **learned time based embeddings** that preserve both numeric magnitude and temporal position. These embeddings are fed into a pre-trained Qwen2.5-0.5B model using `inputs_embeds`, and the transformer backbone is adapted via **LoRA fine-tuning**.\n",
        "\n",
        "**Why:** Embedding time series as tokens makes it possible to leverage the pretrained knowledge and pattern recognition of LLMs while adapting efficiently with just a few trainable parameters (via LoRA).\n",
        "\n",
        "---\n",
        "\n",
        "### TimeAwareEmbed Module\n",
        "\n",
        "The `TimeAwareEmbed` module combines multiple streams of input:\n",
        "\n",
        "- **Value Embeddings**: A 2-layer MLP processes the normalized ticket count for each day.\n",
        "- **Numeric Feature Projections**: Includes lag features, rolling averages, backlog_gap, and resolve_ratio — all passed through a linear layer.\n",
        "- **Categorical Embeddings**: Calendar features like day-of-week, month, weekend flags, end-of-month, and end-of-quarter are embedded.\n",
        "- **Positional Encoding**: Sinusoidal encoding preserves the relative order of the sequence.\n",
        "\n",
        "**Why:** This modular embedding design ensures that each day’s input reflects both historical patterns and operational context, for accurate prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### Feature Engineering\n",
        "\n",
        "Domain-specific features are crafted to reflect customer support dynamics:\n",
        "\n",
        "- **Lag Features**: Capture short-term memory and recency effects.\n",
        "- **Rolling Statistics**: Smooth high variance in daily volumes.\n",
        "- **Backlog Gap**: Measures strain on the support system.\n",
        "- **Resolve Ratio**: Represents operational throughput efficiency.\n",
        "\n",
        "**Why:** These features inject domain knowledge directly into the model, improving generalization especially in sparse or volatile regimes.\n",
        "\n",
        "---\n",
        "\n",
        "### Quantile Forecasting Head\n",
        "\n",
        "After the transformer processes the input sequence, a learned `<PRED>` token is appended. Its hidden state is passed through a small MLP to predict **[P10, P50, P90]** quantiles using the **pinball loss function**.\n",
        "\n",
        "**Why:** Predicting quantiles (instead of just point estimates) gives actionable uncertainty intervals for decisions, making the model robust to outliers and variability.\n",
        "\n",
        "---\n",
        "\n",
        "### Training & Setup\n",
        "\n",
        "- Training windows: **28-day history → next-day forecast**\n",
        "- Evaluation metrics: MAE, RMSE, Coverage@80%\n",
        "- Optimization: **LoRA** fine-tuning (low-rank adapters)\n",
        "- Quantization: Optional **4-bit** (to run on consumer GPUs)\n",
        "- Backbones: TinyLlama (for CPU/Mac), Qwen2.5/Mistral (for CUDA GPUs)\n",
        "\n"
      ],
      "metadata": {
        "id": "vHFbSSb6Nx4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 1) Data Processing"
      ],
      "metadata": {
        "id": "Q7n9_i8jy4Do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# i) Helper Functions\n",
        "\n",
        "> This cell sets up paths, feature definitions, and utility functions, and defines a fit_scalers function to normalize targets and numeric features using a log1p transform for stability.\n",
        "\n",
        "### Numeric features (`NUMERIC_FEATS`)\n",
        "- **tickets_received / tickets_resolved** → raw daily counts; the core signal.  \n",
        "- **lag1, lag7** → ticket volume from 1 day ago or 7 days ago; capture immediate and weekly seasonality.  \n",
        "- **r7, r14** → 7-day and 14-day rolling averages; smooth out short-term fluctuations and highlight weekly/bi-weekly trends.  \n",
        "- **backlog_gap** = avg difference between received and resolved over 7 days; indicates whether unresolved tickets are piling up.  \n",
        "- **resolve_ratio** = resolved ÷ received (7-day sums); shows efficiency of the support team.  \n",
        "\n",
        "### Categorical features (`CATEG_FEATS`)\n",
        "- **dow (day of week)** → captures weekday vs. weekend patterns.  \n",
        "- **dom (day of month)** → useful for month-end surges (e.g., billing cycles).  \n",
        "- **month** → seasonal effects across months.  \n",
        "- **is_weekend** → binary flag for low-activity weekends.  \n",
        "- **eom (end of month)** → spikes in tickets near billing/reporting deadlines.  \n",
        "- **eoq (end of quarter)** → spikes near quarterly cycles, often tied to finance or compliance.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fo2Dy945vliF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "urW2NjhTQjbQ"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "\n",
        "OPEN = \"requests_opened_external.csv\"\n",
        "CLOSED = \"requests_closed_external.csv\"\n",
        "ART_DIR = \"artifacts\"\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Forecasting target and feature names\n",
        "TARGET = \"tickets_received\"\n",
        "NUMERIC_FEATS = [\n",
        "    \"tickets_received\", \"tickets_resolved\",\n",
        "    \"tickets_received_lag1\", \"tickets_received_lag7\", \"tickets_received_r7\", \"tickets_received_r14\",\n",
        "    \"tickets_resolved_lag1\", \"tickets_resolved_lag7\", \"tickets_resolved_r7\", \"tickets_resolved_r14\",\n",
        "    \"backlog_gap\", \"resolve_ratio\"\n",
        "]\n",
        "CATEG_FEATS = [\"dow\", \"dom\", \"month\", \"is_weekend\", \"eom\", \"eoq\"]  # cardinalities: 7,32,12,2,2,2\n",
        "\n",
        "\n",
        "def detect_device():\n",
        "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
        "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available(): return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def save_fig(path):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150)\n",
        "    print(\"Saved figure →\", path)\n",
        "\n",
        "\n",
        "def maybe_prepare_kbit(model):\n",
        "    try:\n",
        "        from peft import prepare_model_for_kbit_training\n",
        "        return prepare_model_for_kbit_training(model)\n",
        "    except Exception:\n",
        "        return model\n",
        "\n",
        "\n",
        "def fit_scalers(train_df):\n",
        "    \"\"\"\n",
        "    Compute mean/std for target and numeric features\n",
        "    so we can normalize consistently.\n",
        "    \"\"\"\n",
        "    # target column - use log1p with NaN handling\n",
        "    y_vals = train_df[TARGET].values.astype(float)\n",
        "    y_vals = np.clip(y_vals, 0, None)  # ensure non-negative before log\n",
        "    logy = np.log1p(y_vals)\n",
        "    y_mean, y_std = logy.mean(), logy.std() + 1e-8\n",
        "\n",
        "    # numeric feature columns - use global NUMERIC_FEATS\n",
        "    num_data = train_df[NUMERIC_FEATS].copy()\n",
        "    # Handle negative values before log1p\n",
        "    num_data = num_data.clip(lower=0)\n",
        "    num_data = num_data.fillna(0)  # handle any remaining NaN\n",
        "\n",
        "    num_means = np.log1p(num_data).mean()\n",
        "    num_stds = np.log1p(num_data).std().replace(0, 1)\n",
        "\n",
        "    return y_mean, y_std, num_means, num_stds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ii) Data Quality - Audit + Clean\n",
        "\n",
        "\n",
        "\n",
        "> Loads raw minute-level opened data, coerces numeric volumes and parsed datetimes, drops invalid rows, clips negatives, and aggregates duplicate timestamps; then resamples to daily (tickets_received), reports missing days, generates quick visuals (last-30d series, histogram, daily + 7d MA, weekday boxplot), saves cleaned minute/daily CSVs, prints a DQ summary, and returns both the cleaned minute-level and daily DataFrames.\n",
        "\n"
      ],
      "metadata": {
        "id": "oGiQGyOtwfS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def dq_report_opened(open_path=OPEN, out_dir=ART_DIR, preview_days=30) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    print(\"\\n=== DQ: Loading raw opened data ===\")\n",
        "    raw = pd.read_csv(open_path)\n",
        "    print(f\"Rows (raw): {len(raw)}\")\n",
        "    cols = {c.lower().strip(): c for c in raw.columns}\n",
        "    need = [\"date\", \"time\", \"volume\"]\n",
        "    for n in need:\n",
        "        if n not in cols:\n",
        "            raise ValueError(f\"Missing expected column '{n}'. Found: {list(raw.columns)}\")\n",
        "    DateCol, TimeCol, VolCol = cols[\"date\"], cols[\"time\"], cols[\"volume\"]\n",
        "\n",
        "\n",
        "    raw[\"_vol\"] = pd.to_numeric(raw[VolCol], errors=\"coerce\")\n",
        "    bad_vol = raw[\"_vol\"].isna().sum()\n",
        "    if bad_vol:\n",
        "        print(f\"[WARN] {bad_vol} rows have non-numeric Volume → will drop.\")\n",
        "\n",
        "\n",
        "    dt = pd.to_datetime(\n",
        "        raw[DateCol].astype(str).str.strip() + \" \" + raw[TimeCol].astype(str).str.strip(),\n",
        "        errors=\"coerce\", format=\"%m/%d/%Y %H:%M\"\n",
        "    )\n",
        "    bad_dt = dt.isna().sum()\n",
        "    if bad_dt:\n",
        "        print(f\"[WARN] {bad_dt} rows have invalid Date/Time → will drop.\")\n",
        "\n",
        "\n",
        "    ro = pd.DataFrame({\"DateTime\": dt, \"Volume\": raw[\"_vol\"]}).dropna(subset=[\"DateTime\",\"Volume\"]).copy()\n",
        "\n",
        "\n",
        "    neg = (ro[\"Volume\"] < 0).sum()\n",
        "    if neg:\n",
        "        print(f\"[WARN] {neg} negative volumes → clipping to 0.\")\n",
        "    ro[\"Volume\"] = ro[\"Volume\"].clip(lower=0)\n",
        "\n",
        "\n",
        "    dupe_counts = ro.duplicated(subset=[\"DateTime\"]).sum()\n",
        "    if dupe_counts:\n",
        "        print(f\"[INFO] {dupe_counts} duplicate minute timestamps → aggregating by sum.\")\n",
        "    ro = ro.groupby(\"DateTime\", as_index=False)[\"Volume\"].sum()\n",
        "\n",
        "\n",
        "    ro = ro.sort_values(\"DateTime\").reset_index(drop=True)\n",
        "    print(f\"Rows (cleaned): {len(ro)} | Range: {ro['DateTime'].min()} → {ro['DateTime'].max()}\")\n",
        "\n",
        "\n",
        "    # Visualizations\n",
        "    recent_start = ro[\"DateTime\"].max() - pd.Timedelta(days=preview_days)\n",
        "    ro_recent = ro[ro[\"DateTime\"] >= recent_start]\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.plot(ro_recent[\"DateTime\"], ro_recent[\"Volume\"])\n",
        "    plt.title(f\"Opened (minute) – last {preview_days} days\"); plt.xlabel(\"Time\"); plt.ylabel(\"Vol/min\")\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    save_fig(os.path.join(out_dir, f\"dq_opened_minute_last{preview_days}d.png\"))\n",
        "\n",
        "\n",
        "    vmax = np.nanpercentile(ro[\"Volume\"], 99.5)\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.hist(ro[\"Volume\"].clip(upper=vmax), bins=50)\n",
        "    plt.title(\"Minute volumes (99.5% clipped)\"); plt.xlabel(\"Vol/min\"); plt.ylabel(\"Count\")\n",
        "    save_fig(os.path.join(out_dir, \"dq_opened_minute_hist.png\"))\n",
        "\n",
        "\n",
        "    opened_daily = ro.set_index(\"DateTime\")[\"Volume\"].resample(\"D\").sum().to_frame(\"tickets_received\")\n",
        "    opened_daily.index.name = \"date\"\n",
        "\n",
        "\n",
        "    idx_full = pd.date_range(opened_daily.index.min(), opened_daily.index.max(), freq=\"D\")\n",
        "    missing_days = idx_full.difference(opened_daily.index)\n",
        "    print(f\"Missing calendar days: {len(missing_days)}\")\n",
        "    if len(missing_days):\n",
        "        print(\"First 10 missing:\", list(missing_days[:10]))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.plot(opened_daily.index, opened_daily[\"tickets_received\"], label=\"Daily sum\")\n",
        "    plt.plot(opened_daily.index, opened_daily[\"tickets_received\"].rolling(7, min_periods=1).mean(), label=\"7d mean\")\n",
        "    plt.title(\"Daily tickets_received\"); plt.xlabel(\"Date\"); plt.ylabel(\"Tickets/day\"); plt.legend()\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    save_fig(os.path.join(out_dir, \"dq_opened_daily_line.png\"))\n",
        "\n",
        "\n",
        "    ddf = opened_daily.copy()\n",
        "    ddf[\"dow\"] = ddf.index.dayofweek\n",
        "    data_by_dow = [ddf[ddf[\"dow\"]==i][\"tickets_received\"].values for i in range(7)]\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.boxplot(data_by_dow, tick_labels=[\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"], showfliers=False)\n",
        "    plt.title(\"Tickets by weekday\"); plt.ylabel(\"Tickets/day\")\n",
        "    save_fig(os.path.join(out_dir, \"dq_opened_daily_box_by_dow.png\"))\n",
        "\n",
        "\n",
        "    minute_clean_path = os.path.join(out_dir, \"requests_opened_external_clean.csv\")\n",
        "    daily_clean_path  = os.path.join(out_dir, \"opened_daily_clean.csv\")\n",
        "    ro.to_csv(minute_clean_path, index=False)\n",
        "    opened_daily.to_csv(daily_clean_path)\n",
        "    print(\"Saved cleaned minute →\", minute_clean_path)\n",
        "    print(\"Saved cleaned daily  →\", daily_clean_path)\n",
        "\n",
        "\n",
        "    print(\"\\n=== DQ Summary ===\")\n",
        "    print(f\"- Dropped invalid Date/Time rows: {bad_dt}\")\n",
        "    print(f\"- Dropped non-numeric Volume rows: {bad_vol}\")\n",
        "    print(f\"- Negative volumes clipped: {neg}\")\n",
        "    print(f\"- Duplicate minutes aggregated: {dupe_counts}\")\n",
        "    print(f\"- Missing calendar days: {len(missing_days)}\")\n",
        "    return ro, opened_daily"
      ],
      "metadata": {
        "id": "Co3Ql3FAthqh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## iii) Merge + Feature Engineering (daily)\n",
        "\n",
        "- **`load_merge_daily`**  \n",
        "  - Aggregates opened tickets to daily counts.  \n",
        "  - Clips extreme outliers at the 98th percentile.  \n",
        "  - Aligns with closed tickets dataset on a full daily index.  \n",
        "  - Fills gaps (≤7 days) with interpolation + forward/backward fill.  \n",
        "  - Clips any negative values to 0.  \n",
        "\n",
        "- **`make_features`**  \n",
        "  - Adds calendar features: day-of-week, day-of-month, month, weekend flag, end-of-month, end-of-quarter.  \n",
        "  - Creates lag features (`lag1`, `lag7`) and rolling means (`r7`, `r14`) for opened/closed tickets.  \n",
        "  - Builds derived features: backlog gap and resolve ratio.  \n",
        "  - Fills NaNs, runs validation checks, and returns the clean feature matrix.  \n"
      ],
      "metadata": {
        "id": "GoZe3bYTyVbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_merge_daily(open_path=OPEN, closed_path=CLOSED, clip_pct=0.98):  # More aggressive clipping\n",
        "    ro = pd.read_csv(open_path)\n",
        "    rc = pd.read_csv(closed_path)\n",
        "\n",
        "    ro[\"DateTime\"] = pd.to_datetime(ro[\"Date\"] + \" \" + ro[\"Time\"], format=\"%m/%d/%Y %H:%M\")\n",
        "    opened_daily = ro.set_index(\"DateTime\")[\"Volume\"].resample(\"D\").sum().to_frame(\"tickets_received\")\n",
        "    opened_daily.index.name = \"date\"\n",
        "\n",
        "    # More aggressive clipping and logging\n",
        "    before_clip = opened_daily[\"tickets_received\"].max()\n",
        "    q = opened_daily[\"tickets_received\"].quantile(clip_pct)\n",
        "    opened_daily[\"tickets_received\"] = opened_daily[\"tickets_received\"].clip(upper=q)\n",
        "    after_clip = opened_daily[\"tickets_received\"].max()\n",
        "    print(f\"[INFO] Clipped daily max from {before_clip:.0f} to {after_clip:.0f} at {clip_pct:.1%} quantile\")\n",
        "\n",
        "    rc[\"date\"] = pd.to_datetime(rc[\"date\"], format=\"%Y-%m-%d\")\n",
        "    closed_daily = rc.rename(columns={\"volume\": \"tickets_resolved\"}).set_index(\"date\")\n",
        "\n",
        "    idx = pd.date_range(start=min(opened_daily.index.min(), closed_daily.index.min()),\n",
        "                        end=max(opened_daily.index.max(), closed_daily.index.max()), freq=\"D\")\n",
        "    df = opened_daily.reindex(idx).join(closed_daily.reindex(idx), how=\"outer\")\n",
        "    df = df.interpolate(limit=7, limit_direction=\"both\").ffill().bfill()\n",
        "    df[\"tickets_received\"] = df[\"tickets_received\"].clip(lower=0)\n",
        "    df[\"tickets_resolved\"] = df[\"tickets_resolved\"].clip(lower=0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df.copy()\n",
        "\n",
        "    # Calendar categoricals (zero-based + clipped)\n",
        "    d[\"dow\"]   = d.index.dayofweek.astype(int)                          # 0..6\n",
        "    d[\"dom\"]   = np.clip(d.index.day.astype(int)   - 1, 0, 30).astype(int)  # 0..30\n",
        "    d[\"month\"] = np.clip(d.index.month.astype(int) - 1, 0, 11).astype(int)  # 0..11\n",
        "\n",
        "    # Binary calendar flags\n",
        "    d[\"is_weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
        "    d[\"eom\"]        = d.index.is_month_end.astype(int)\n",
        "    d[\"eoq\"]        = d.index.is_quarter_end.astype(int)\n",
        "\n",
        "    # Lag/rolling features with better NaN handling\n",
        "    for col in [\"tickets_received\", \"tickets_resolved\"]:\n",
        "        d[f\"{col}_lag1\"] = d[col].shift(1)\n",
        "        d[f\"{col}_lag7\"] = d[col].shift(7)\n",
        "        d[f\"{col}_r7\"]   = d[col].rolling(7,  min_periods=1).mean()\n",
        "        d[f\"{col}_r14\"]  = d[col].rolling(14, min_periods=1).mean()\n",
        "\n",
        "    # Backlog & ratio features\n",
        "    d[\"backlog_gap\"] = (d[\"tickets_received\"] - d[\"tickets_resolved\"]).rolling(7, min_periods=1).mean()\n",
        "    d[\"resolve_ratio\"] = (d[\"tickets_resolved\"].rolling(7, min_periods=1).sum() + 1) / \\\n",
        "                         (d[\"tickets_received\"].rolling(7, min_periods=1).sum() + 1)\n",
        "\n",
        "    # Fill any remaining NaN values before dropping\n",
        "    d = d.fillna(method='ffill').fillna(0)\n",
        "\n",
        "    # Validation checks\n",
        "    print(f\"[INFO] Features shape before dropna: {d.shape}\")\n",
        "    print(f\"[INFO] NaN count by column: {d.isnull().sum().sum()}\")\n",
        "\n",
        "    return d.dropna()"
      ],
      "metadata": {
        "id": "RSbw81F9tXJf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## iv) Dataset & Scalers\n",
        "\n",
        "- **Target scaling**: `norm_y` applies `log1p` + z-normalization; `denorm_y` reverses it.  \n",
        "- **Feature scaling**: `transform_numeric` clips negatives, fills NaNs, applies `log1p`, then standardizes with precomputed means/stds.  \n",
        "- **`TSDataset`**: builds sliding windows of **28 days** (`window`) to predict the **next day** (`horizon=1`), returning numeric features, categorical features, normalized past targets, and the next-day target.  \n",
        "- **Pipeline**:  \n",
        "  - Load merged daily data → build engineered features.  \n",
        "  - Split 80/20 into train/validation sets.  \n",
        "  - Fit scalers on training data.  \n",
        "  - Create `train_ds` / `valid_ds` and wrap them in **DataLoaders** (batch=32).  \n",
        "- **Output**: ~10k days total → ~8k training samples, ~2k validation samples, with logs showing target ranges and normalization stats.  \n"
      ],
      "metadata": {
        "id": "CtYieyR4ydQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "def norm_y(y, y_mean, y_std):\n",
        "    y_safe = np.clip(y.astype(np.float32), 0, None)  # ensure non-negative\n",
        "    return (np.log1p(y_safe) - y_mean) / y_std\n",
        "\n",
        "def denorm_y(y, y_mean, y_std):\n",
        "    return np.expm1(y * y_std + y_mean)\n",
        "\n",
        "\n",
        "# def norm_y(y, y_mean=None, y_std=None):\n",
        "#     return np.log1p(y.astype(np.float32))\n",
        "\n",
        "# def denorm_y(y, y_mean=None, y_std=None):\n",
        "#     return np.expm1(y)\n",
        "\n",
        "# def fit_scalers(train_df):\n",
        "#     # Return dummy values since we're not using them\n",
        "#     return 0.0, 1.0, num_means, num_stds  # Keep numeric feature scaling\n",
        "\n",
        "# def fit_scalers(train_df: pd.DataFrame):\n",
        "#     # No longer need y_mean, y_std for target\n",
        "#     y_mean, y_std = 0.0, 1.0  # dummy values\n",
        "\n",
        "#     # Still need numeric feature scaling\n",
        "#     num_data = train_df[NUMERIC_FEATS].copy()\n",
        "#     num_data = num_data.clip(lower=0).fillna(0)\n",
        "#     num_means = np.log1p(num_data).mean()\n",
        "#     num_stds = np.log1p(num_data).std().replace(0, 1)\n",
        "\n",
        "#     return y_mean, y_std, num_means, num_stds\n",
        "\n",
        "def transform_numeric(df_part, num_means, num_stds):\n",
        "    # Clip and fill before log transformation\n",
        "    X = df_part[NUMERIC_FEATS].copy()\n",
        "    X = X.clip(lower=0).fillna(0)\n",
        "    X = np.log1p(X)\n",
        "    X = (X - num_means) / (num_stds + 1e-8)\n",
        "    return X.astype(np.float32).values\n",
        "\n",
        "\n",
        "class TSDataset(Dataset):\n",
        "    def __init__(self, df_part, y_mean, y_std, num_means, num_stds, window=28, horizon=1):\n",
        "        self.df = df_part\n",
        "        self.window = window; self.horizon = horizon\n",
        "        self.numeric = transform_numeric(df_part, num_means, num_stds)\n",
        "        self.cats = df_part[CATEG_FEATS].astype(int).values\n",
        "        self.target = df_part[TARGET].values.astype(np.float32)\n",
        "        self.y_mean, self.y_std = y_mean, y_std\n",
        "        self.idx = [i for i in range(window, len(df_part)-horizon)]\n",
        "\n",
        "        # Validation\n",
        "        print(f\"[INFO] Dataset created: {len(self.idx)} samples, target range: {self.target.min():.1f}-{self.target.max():.1f}\")\n",
        "\n",
        "    def __len__(self): return len(self.idx)\n",
        "    def __getitem__(self, i):\n",
        "        t = self.idx[i]; s = t - self.window; e = t\n",
        "        X_num = torch.tensor(self.numeric[s:e], dtype=torch.float32)\n",
        "        X_cat = torch.tensor(self.cats[s:e], dtype=torch.long)\n",
        "        val_series = self.target[s:e]\n",
        "        val_norm = torch.tensor(norm_y(val_series, self.y_mean, self.y_std), dtype=torch.float32)\n",
        "        y_next = torch.tensor(norm_y(self.target[t:t+self.horizon], self.y_mean, self.y_std), dtype=torch.float32)\n",
        "        return {\"X_num\": X_num, \"X_cat\": X_cat, \"val_norm\": val_norm, \"y_next\": y_next}\n",
        "\n",
        "\n",
        "# Load and process data\n",
        "df_daily = load_merge_daily()\n",
        "feats = make_features(df_daily)\n",
        "print(\"Feature frame shape:\", feats.shape)\n",
        "\n",
        "# Split and build loaders\n",
        "WINDOW, HORIZON = 28, 1\n",
        "split = int(len(feats)*0.8)\n",
        "train_df, valid_df = feats.iloc[:split], feats.iloc[split:]\n",
        "\n",
        "print(f\"[INFO] Train target stats: min={train_df[TARGET].min():.1f}, max={train_df[TARGET].max():.1f}, mean={train_df[TARGET].mean():.1f}\")\n",
        "print(f\"[INFO] Valid target stats: min={valid_df[TARGET].min():.1f}, max={valid_df[TARGET].max():.1f}, mean={valid_df[TARGET].mean():.1f}\")\n",
        "\n",
        "y_mean, y_std, num_means, num_stds = fit_scalers(train_df)\n",
        "print(f\"[INFO] Normalization params: y_mean={y_mean:.3f}, y_std={y_std:.3f}\")\n",
        "\n",
        "train_ds = TSDataset(train_df, y_mean, y_std, num_means, num_stds, window=WINDOW, horizon=HORIZON)\n",
        "valid_ds = TSDataset(valid_df, y_mean, y_std, num_means, num_stds, window=WINDOW, horizon=HORIZON)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI7NxFeQtaqI",
        "outputId": "380a03d1-a74c-4d9d-e50d-05685ae23c7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Clipped daily max from 134993461 to 24344049 at 98.0% quantile\n",
            "[INFO] Features shape before dropna: (10045, 18)\n",
            "[INFO] NaN count by column: 0\n",
            "Feature frame shape: (10045, 18)\n",
            "[INFO] Train target stats: min=0.0, max=24344048.8, mean=6837270.6\n",
            "[INFO] Valid target stats: min=0.0, max=24344048.8, mean=2241675.8\n",
            "[INFO] Normalization params: y_mean=10.978, y_std=7.395\n",
            "[INFO] Dataset created: 8007 samples, target range: 0.0-24344048.0\n",
            "[INFO] Dataset created: 1980 samples, target range: 0.0-24344048.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-177649642.py:54: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  d = d.fillna(method='ffill').fillna(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v) Category Sizes\n",
        "\n",
        "- Computes the number of unique values for each categorical feature, to define embedding dimensions later.  \n",
        "- Results:  \n",
        "  - `dow`: 7 (days of week)  \n",
        "  - `dom`: 31 (days of month)  \n",
        "  - `month`: 12 (months)  \n",
        "  - `is_weekend`: 2  \n",
        "  - `eom`: 2 (end-of-month flag)  \n",
        "  - `eoq`: 2 (end-of-quarter flag)  \n",
        "- **Output**: `[7, 31, 12, 2, 2, 2]` → used to size categorical embedding layers.  \n"
      ],
      "metadata": {
        "id": "Cd6RZ221yoGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBqmJUBXrC4",
        "outputId": "959b6ff0-559f-440c-8870-75a151d32cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat_sizes: [7, 31, 12, 2, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "cat_sizes = [\n",
        "    feats[\"dow\"].max() + 1,     # 7\n",
        "    feats[\"dom\"].max() + 1,     # 31\n",
        "    feats[\"month\"].max() + 1,   # 12\n",
        "    feats[\"is_weekend\"].max() + 1,  # 2\n",
        "    feats[\"eom\"].max() + 1,         # 2\n",
        "    feats[\"eoq\"].max() + 1,         # 2\n",
        "]\n",
        "print(\"cat_sizes:\", cat_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Data Quality Findings & Decisions\n",
        "\n",
        "**Issue 1: Extreme Outliers (135M → 24M tickets/day)**\n",
        "- Decision: Clip at 98th percentile rather than 99.5%\n",
        "- Rationale: Values >100M appear to be data entry errors rather than genuine volume spikes\n",
        "- Impact: Removes 2% of extreme values while preserving realistic high-volume periods\n",
        "\n",
        "**Issue 2: Weekend vs Weekday Patterns**\n",
        "- Finding: Weekends show near-zero activity.\n",
        "- Decision: Preserve this pattern rather than smooth it\n",
        "- Rationale: Reflects genuine operational reality for staffing decisions\n",
        "\n",
        "**Issue 3: Feature Normalization Failures**\n",
        "\n",
        "Finding: Multiple RuntimeWarning: invalid value encountered in log1p during feature scaling\n",
        "Root cause: Negative values in lag/rolling features creating NaN after log transformation\n",
        "Decision: Added explicit .clip(lower=0) before log1p transformation\n",
        "Rationale: Negative \"resolved tickets\" don't make business sense; likely interpolation artifacts\n",
        "\n",
        "**Issue 4: Z-Score Normalization Amplification Problem**\n",
        "\n",
        "Finding: Small prediction errors in normalized space (±0.1) became massive real-world errors (millions of tickets)\n",
        "Analysis: expm1(z * 7.4 + 11.0) exponential denormalization amplifies any prediction uncertainty\n",
        "Decision: Acknowledged limitation rather than masking it with artificial constraints\n",
        "Alternative considered: Direct log-space prediction (tested but showed poor learning)\n",
        "\n",
        "**Issue 5: Target Distribution Extreme Skewness**\n",
        "\n",
        "Finding: Target values spanning 0 to 135M+ tickets (8+ orders of magnitude)\n",
        "Investigation: Single day spike to 135M represents ~4x normal maximum volume\n",
        "Decision: Aggressive 98% clipping rather than 99.5%\n",
        "Rationale: Balance between preserving genuine high-volume events vs removing apparent data errors\n",
        "Impact: Reduced max from 135M to 24M while retaining realistic operational range\n",
        "\n",
        "**Issue 6: Time Series Continuity Problems**\n",
        "\n",
        "Finding: Minute-level data required aggregation to daily, but no missing timestamps found\n",
        "Decision: Simple daily sum aggregation rather than complex weighted averaging\n",
        "Validation: Confirmed 0 missing calendar days in final dataset\n",
        "Business logic: Daily totals more relevant for staffing than intraday patterns\n",
        "\n",
        "**Issue 7: Cross-Dataset Integration Challenges**\n",
        "\n",
        "Finding: Different date formats between opened (M/d/Y H:M) and closed (Y-m-d) datasets\n",
        "Decision: Standardize both to daily pandas DatetimeIndex for alignment\n",
        "Gap handling: Interpolate missing closed ticket data with 7-day limit\n",
        "Rationale: Closed tickets less critical than opened for forecasting, but useful for operational features\n",
        "\n",
        "**Issue 8: Feature Engineering Validation**\n",
        "\n",
        "Finding: Initial feature definitions in fit_scalers() didn't match actual engineered features\n",
        "Problem: Hardcoded feature list caused crashes during training\n",
        "Decision: Refactored to use global NUMERIC_FEATS consistently\n",
        "Prevention: Added validation prints showing feature shapes and NaN counts\n",
        "\n",
        "**Issue 9: Model Architecture vs Data Scale Mismatch**\n",
        "\n",
        "Finding: 500M parameter model for 8K training samples (62,500:1 parameter-to-sample ratio)\n",
        "Analysis: Massive overparameterization typically leads to overfitting\n",
        "Mitigation: LoRA fine-tuning (only 0.88% parameters trainable) + aggressive regularization\n",
        "Monitoring: Close validation performance to training indicates acceptable generalization\n",
        "\n",
        "**Issue 10: Evaluation Pipeline Architectural Decisions**\n",
        "\n",
        "Challenge: How to evaluate a model trained in normalized space against business requirements\n",
        "Decision: Dual evaluation approach - normalized space (for model assessment) + denormalized space (for baseline comparison)\n",
        "Trade-off: Accurate model performance measurement vs interpretable business metrics\n",
        "Documentation: Explicit acknowledgment that current pipeline creates evaluation challenges for production deployment\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EDADokCiTXfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Model Architechture"
      ],
      "metadata": {
        "id": "HQz3YjLxzBuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## i) Model: Time-Aware Embeddings → LLM Backbone → Quantile Forecasts\n",
        "\n",
        "- **`TimeAwareEmbed`**: builds per-day embeddings by combining (a) `value_mlp(val_norm)` for past target, (b) linear projection of **numeric features**, (c) summed **categorical embeddings** (sizes from `cat_sizes`), plus **sinusoidal positional encodings**; then applies `LayerNorm`, dropout, and NaN/Inf guards.\n",
        "- **`QuantileHead`**: MLP that outputs ordered **p10/p50/p90** via cumulative softplus widths (`a`, `a+softplus(b)`, `…+softplus(c)`) to guarantee monotonic quantiles; clamps to a safe range.\n",
        "- **`TSLLM`**: adapts a causal LLM by feeding **inputs_embeds** (time-aware sequence + learned `<PRED>` token), scales with `input_gain`, requests **hidden states**, and reads the final token’s hidden vector to the quantile head.\n",
        "- **Losses**: `pinball_loss` for quantile regression; `safe_pinball_loss` adds sanitization, clamping, and enforced monotonicity for stable training.\n"
      ],
      "metadata": {
        "id": "eWwNh867zEWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mh4U4r8YXBmZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TimeAwareEmbed(nn.Module):\n",
        "    def __init__(self, d_model: int, num_numeric: int, cat_sizes=(7,32,12,2,2,2), p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.value_mlp = nn.Sequential(\n",
        "            nn.Linear(1, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "        self.num_proj  = nn.Linear(num_numeric, d_model)\n",
        "        self.cat_embs  = nn.ModuleList([nn.Embedding(s, d_model) for s in cat_sizes])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, val_norm, num_feats, cat_feats):\n",
        "        B, W, _ = num_feats.shape\n",
        "\n",
        "        # categorical range guard\n",
        "        for i, emb in enumerate(self.cat_embs):\n",
        "            mx = int(cat_feats[..., i].max().item()); mn = int(cat_feats[..., i].min().item())\n",
        "            if mx >= emb.num_embeddings or mn < 0:\n",
        "                raise ValueError(f\"Cat feat {i} out of range [{mn},{mx}] vs size {emb.num_embeddings}\")\n",
        "\n",
        "        v_emb = self.value_mlp(val_norm.view(B, W, 1))\n",
        "        n_emb = self.num_proj(num_feats)\n",
        "        c_sum = 0\n",
        "        for i, emb in enumerate(self.cat_embs):\n",
        "            c_sum = c_sum + emb(cat_feats[..., i])\n",
        "\n",
        "        # sinusoidal positional encoding\n",
        "        device = num_feats.device\n",
        "        pos = torch.arange(W, device=device).unsqueeze(1).float()\n",
        "        i = torch.arange(self.d_model, device=device).float().unsqueeze(0)\n",
        "        angle = pos * (1/torch.pow(10000, (2*(i//2))/self.d_model))\n",
        "        pe = torch.zeros(W, self.d_model, device=device)\n",
        "        pe[:, 0::2] = torch.sin(angle[:, 0::2]); pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "\n",
        "        x = v_emb + n_emb + c_sum + pe.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        # numeric stability: replace NaN/Inf, scale, norm, and drop\n",
        "        x = torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        x = x * (1.0 / math.sqrt(self.d_model))\n",
        "        x = self.ln(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class QuantileHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Monotonic head with cumulative softplus widths:\n",
        "      p10 = a\n",
        "      p50 = a + softplus(b) + eps\n",
        "      p90 = p50 + softplus(c) + eps\n",
        "    This guarantees ordered quantiles and non-zero width.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int, horizon=1, quantiles=(0.1,0.5,0.9), eps=1e-3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        hs = hidden_size\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hs, 2*hs), nn.GELU(), nn.Dropout(p_drop),\n",
        "            nn.Linear(2*hs, hs), nn.GELU(), nn.Dropout(p_drop),\n",
        "        )\n",
        "        self.out = nn.Linear(hs, horizon*3)   # (a, b, c) per step\n",
        "        self.horizon = horizon\n",
        "        self.eps = eps\n",
        "\n",
        "        # init: small random, slight positive widths so we don't start with zero interval\n",
        "        nn.init.zeros_(self.out.bias)\n",
        "        nn.init.normal_(self.out.weight, mean=0.0, std=0.01)\n",
        "\n",
        "    def forward(self, hidden):\n",
        "        h = self.mlp(hidden)\n",
        "        raw = self.out(h).view(-1, self.horizon, 3)  # [..., (a,b,c)]\n",
        "\n",
        "        a = raw[..., 0]\n",
        "        b = F.softplus(raw[..., 1]) + self.eps\n",
        "        c = F.softplus(raw[..., 2]) + self.eps\n",
        "\n",
        "        p10 = a\n",
        "        p50 = a + b\n",
        "        p90 = p50 + c\n",
        "\n",
        "        pred = torch.stack([p10, p50, p90], dim=-1)  # [B, H, 3]\n",
        "        # absolute guard in z-space\n",
        "        pred = torch.nan_to_num(pred, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        pred = torch.clamp(pred, -10.0, 10.0)\n",
        "        return pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ii) TSLLM + Quantile Losses\n",
        "\n",
        "- **`TSLLM`**: Feeds time-aware embeddings plus a learned `<PRED>` token into the LLM backbone via `inputs_embeds`, scales with `input_gain`, requests hidden states, and takes the final token’s hidden vector to a `QuantileHead` that outputs ordered p10/p50/p90 forecasts.\n",
        "- **Losses**: `pinball_loss` does standard quantile regression; `safe_pinball_loss` adds NaN/Inf sanitization, clamps values, and enforces monotonic quantiles for stable training.\n"
      ],
      "metadata": {
        "id": "-Hx0N6eGzRQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TSLLM(nn.Module):\n",
        "    def __init__(self, base_causal_lm, hidden_size: int,\n",
        "                 num_numeric: int, cat_sizes=(7,32,12,2,2,2),\n",
        "                 horizon=1, quantiles=(0.1,0.5,0.9)):\n",
        "        super().__init__()\n",
        "        # keep a handle to the full module (could be PEFT-wrapped)\n",
        "        self.full = base_causal_lm\n",
        "        # try to grab the underlying backbone if exposed; otherwise use the module itself\n",
        "        self.backbone = getattr(base_causal_lm, \"model\", None)\n",
        "        if self.backbone is None:\n",
        "            self.backbone = getattr(base_causal_lm, \"base_model\", base_causal_lm)\n",
        "\n",
        "        self.embedder = TimeAwareEmbed(hidden_size, num_numeric, cat_sizes)\n",
        "        self.pred_tok = nn.Parameter(torch.randn(1,1,hidden_size)*0.02)\n",
        "        self.head = QuantileHead(hidden_size, horizon=horizon, quantiles=quantiles)\n",
        "        self.input_gain = nn.Parameter(torch.tensor(5.0))\n",
        "\n",
        "    def forward(self, val_norm, X_num, X_cat):\n",
        "        # build sequence of learned time-aware embeddings\n",
        "        x = self.embedder(val_norm, X_num, X_cat)                 # [B, W, d]\n",
        "        B = x.size(0)\n",
        "        seq = torch.cat([x, self.pred_tok.repeat(B,1,1)], dim=1) # [B, W+1, d]\n",
        "        seq = self.input_gain * seq\n",
        "        # causal mask is handled internally by decoder-only models; we just give a basic attention mask\n",
        "        attn_mask = torch.ones(seq.size()[:2], dtype=torch.long, device=seq.device)\n",
        "\n",
        "\n",
        "        out = self.backbone(\n",
        "            inputs_embeds=seq,\n",
        "            attention_mask=attn_mask,\n",
        "            use_cache=False,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        # Robustly get the final hidden representation of the <PRED> token\n",
        "        if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
        "            hidden_step = out.last_hidden_state[:, -1, :]   # [B, d]\n",
        "        elif hasattr(out, \"hidden_states\") and out.hidden_states is not None:\n",
        "            hidden_step = out.hidden_states[-1][:, -1, :]   # [B, d]\n",
        "        else:\n",
        "            # If the model still doesn't return hidden states, raise a helpful error.\n",
        "            raise RuntimeError(\n",
        "                \"Backbone did not return hidden states. \"\n",
        "                \"Ensure `output_hidden_states=True` and that you're calling the base model.\"\n",
        "            )\n",
        "\n",
        "        return self.head(hidden_step)\n",
        "\n",
        "\n",
        "def pinball_loss(pred, y, quantiles=(0.1,0.5,0.9)):\n",
        "    diff = y.unsqueeze(-1) - pred\n",
        "    parts = [torch.maximum(q*diff[...,i], (q-1)*diff[...,i]) for i,q in enumerate(quantiles)]\n",
        "    return torch.mean(torch.stack(parts, dim=-1))\n",
        "\n",
        "def safe_pinball_loss(pred, y, quantiles=(0.1, 0.5, 0.9)):\n",
        "    \"\"\"\n",
        "    Robust pinball loss:\n",
        "      - sanitizes NaN/Inf\n",
        "      - clamps normalized space to [-10, 10] (prevents exp overflow later)\n",
        "      - enforces monotonic quantiles (P10<=P50<=P90) to keep gradients sane\n",
        "    pred: [B, H, Q], y: [B, H]\n",
        "    \"\"\"\n",
        "    # sanitize\n",
        "    pred = torch.nan_to_num(pred, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    y    = torch.nan_to_num(y,    nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    # clamp in normalized space\n",
        "    pred = torch.clamp(pred, -10.0, 10.0)\n",
        "    y    = torch.clamp(y,   -10.0, 10.0)\n",
        "\n",
        "    # enforce monotonic quantiles\n",
        "    pred, _ = torch.sort(pred, dim=-1)\n",
        "\n",
        "    u = y.unsqueeze(-1) - pred  # [B, H, Q]\n",
        "    losses = []\n",
        "    for i, q in enumerate(sorted(quantiles)):\n",
        "        left  = q * u[..., i]\n",
        "        right = (q - 1.0) * u[..., i]\n",
        "        term = torch.maximum(left, right)\n",
        "        term = torch.nan_to_num(term, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        losses.append(term.mean())\n",
        "    return sum(losses) / len(losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "ibcZWfVCzOVW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## iii) Build & Train Setup\n",
        "\n",
        "- **Backbone & PEFT**: Loads a small causal LM (`Qwen2.5-0.5B-Instruct` by default; optional Mistral + 4-bit), applies **LoRA** to attention/MLP proj layers, and reports trainable params.\n",
        "- **TSLLM wrapper**: Instantiates `TSLLM` with `hidden_size` from config, numeric feature count, and computed `cat_sizes`; forecasts `p10/p50/p90` for horizon=1.\n",
        "- **Optimizer**: AdamW on trainable (LoRA) params with `lr=2e-4`, `weight_decay=1e-4`.\n",
        "- **`evaluate(...)`**: Runs model in eval mode, clamps/sanitizes quantiles, **de-normalizes** outputs, filters non-finite values, and computes **MAE**, **RMSE**, and **P10–P90 coverage**; also returns arrays for plotting.\n"
      ],
      "metadata": {
        "id": "uioQNqZiziQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfTzRusxXD_q",
        "outputId": "7aadfb4b-9848-4891-ea37-9993852c7273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "Model loading complete\n"
          ]
        }
      ],
      "source": [
        "\n",
        "use_mistral = False\n",
        "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\" if not use_mistral else \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "use_4bit = False if not use_mistral else True\n",
        "\n",
        "device = detect_device()\n",
        "print(\"Device:\", device)\n",
        "cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "load_kwargs = dict(trust_remote_code=True)\n",
        "if use_4bit and device.type == \"cuda\":\n",
        "    load_kwargs.update(dict(device_map=\"auto\", load_in_4bit=True))\n",
        "base_lm = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)\n",
        "if use_4bit and device.type == \"cuda\":\n",
        "    base_lm = maybe_prepare_kbit(base_lm)\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "base_lm = get_peft_model(base_lm, lora)\n",
        "base_lm.print_trainable_parameters()\n",
        "\n",
        "ts_model = TSLLM(\n",
        "    base_causal_lm=base_lm,\n",
        "    hidden_size=cfg.hidden_size,\n",
        "    num_numeric=len(NUMERIC_FEATS),\n",
        "    cat_sizes=cat_sizes,           # <--- use sanitized, computed sizes\n",
        "    horizon=1, quantiles=(0.1,0.5,0.9)\n",
        ").to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, ts_model.parameters()),\n",
        "    lr=2e-4, weight_decay=1e-4\n",
        ")\n",
        "\n",
        "def evaluate(model, loader, y_mean, y_std, device):\n",
        "    model.eval()\n",
        "    pq_list, yt_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for b in loader:\n",
        "            out = model(\n",
        "                b[\"val_norm\"].to(device),\n",
        "                b[\"X_num\"].to(device),\n",
        "                b[\"X_cat\"].to(device),\n",
        "            )\n",
        "            # Stabilize normalized quantiles before de-normalization\n",
        "            out = torch.clamp(out, -10.0, 10.0)                     # <- important\n",
        "            out = torch.nan_to_num(out, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "            pq_list.append(out.cpu().numpy())\n",
        "            yt_list.append(b[\"y_next\"].cpu().numpy())\n",
        "\n",
        "    pq = np.concatenate(pq_list, 0)   # [N, H, 3]\n",
        "    yt = np.concatenate(yt_list, 0)   # [N, H]\n",
        "\n",
        "    # squeeze horizon=1\n",
        "    p10n = pq[..., 0].reshape(-1)\n",
        "    p50n = pq[..., 1].reshape(-1)\n",
        "    p90n = pq[..., 2].reshape(-1)\n",
        "    yn   = yt.reshape(-1)\n",
        "\n",
        "    # de-normalize\n",
        "    p10 = denorm_y(p10n, y_mean, y_std)\n",
        "    p50 = denorm_y(p50n, y_mean, y_std)\n",
        "    p90 = denorm_y(p90n, y_mean, y_std)\n",
        "    y   = denorm_y(yn,   y_mean, y_std)\n",
        "\n",
        "    # mask out non-finite values so sklearn never sees NaN/Inf\n",
        "    mask = np.isfinite(p10) & np.isfinite(p50) & np.isfinite(p90) & np.isfinite(y)\n",
        "    if mask.sum() < mask.size:\n",
        "        bad = mask.size - mask.sum()\n",
        "        print(f\"[evaluate] filtered {bad}/{mask.size} non-finite points (after denorm).\")\n",
        "        # optional quick counts:\n",
        "        # print(\"finite:\", dict(y=np.isfinite(y).sum(), p10=np.isfinite(p10).sum(),\n",
        "        #                      p50=np.isfinite(p50).sum(), p90=np.isfinite(p90).sum()))\n",
        "\n",
        "    y_f, p10_f, p50_f, p90_f = y[mask], p10[mask], p50[mask], p90[mask]\n",
        "    mae = mean_absolute_error(y_f, p50_f)\n",
        "    mse = mean_squared_error(y_f, p50_f)   # works on all versions\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    cov  = np.mean((y_f >= p10_f) & (y_f <= p90_f)) if len(y_f) else np.nan\n",
        "    return mae, rmse, cov, y_f, p50_f, p10_f, p90_f\n",
        "\n",
        "print(\"Model loading complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmxkwveTXGJx",
        "outputId": "5913e42f-5e46-4928-8e7b-c2dbf1cb3c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out shape: (32, 1, 3)\n",
            "any NaN in raw out? False any Inf? False\n"
          ]
        }
      ],
      "source": [
        "# Sanity check: a single forward + finiteness\n",
        "b = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    out = ts_model(b[\"val_norm\"].to(device), b[\"X_num\"].to(device), b[\"X_cat\"].to(device))\n",
        "print(\"out shape:\", tuple(out.shape))\n",
        "print(\"any NaN in raw out?\", torch.isnan(out).any().item(),\n",
        "      \"any Inf?\", torch.isinf(out).any().item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## iv) Normalized Evaluation (helper function)\n",
        "\n",
        "Evaluates the model **in training (z-) space**: gets quantile preds (p10/p50/p90) and targets already normalized, sanitizes/clamps/sorts them, then reports **MAE_z**, **RMSE_z**, and **80% coverage** without de-normalizing. Returns arrays for optional plots/diagnostics.\n"
      ],
      "metadata": {
        "id": "jJCu33v10Mnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ha7YGKAdWbR0"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# Normalized evaluation\n",
        "# =======================\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_normalized(model, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate in the SAME normalized space used during training:\n",
        "      - predictions are the quantiles in z-space (normalized log1p units)\n",
        "      - target is y_next already normalized by the dataset class\n",
        "    Reports: MAE_z, RMSE_z, and coverage (80% PI) all in normalized units.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    preds, ys = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for b in loader:\n",
        "            out = model(\n",
        "                b[\"val_norm\"].to(device),  # [B, W]\n",
        "                b[\"X_num\"].to(device),     # [B, W, F_num]\n",
        "                b[\"X_cat\"].to(device),     # [B, W, F_cat]\n",
        "            )\n",
        "            # keep things safe in z-space\n",
        "            out = torch.nan_to_num(out, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "            out = torch.clamp(out, -10.0, 10.0)\n",
        "            # enforce monotonic quantiles (P10<=P50<=P90)\n",
        "            out, _ = torch.sort(out, dim=-1)\n",
        "\n",
        "            preds.append(out.cpu().numpy())              # [B, H, 3]\n",
        "            ys.append(b[\"y_next\"].cpu().numpy())         # [B, H]\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)               # [N, H, 3]\n",
        "    ys    = np.concatenate(ys, axis=0)                  # [N, H]\n",
        "\n",
        "    # collapse horizon=1\n",
        "    p10n = preds[..., 0].reshape(-1)\n",
        "    p50n = preds[..., 1].reshape(-1)\n",
        "    p90n = preds[..., 2].reshape(-1)\n",
        "    yn   = ys.reshape(-1)\n",
        "\n",
        "    # mask any stragglers\n",
        "    m = np.isfinite(p10n) & np.isfinite(p50n) & np.isfinite(p90n) & np.isfinite(yn)\n",
        "    p10n, p50n, p90n, yn = p10n[m], p50n[m], p90n[m], yn[m]\n",
        "\n",
        "    mae_z  = mean_absolute_error(yn, p50n)\n",
        "    rmse_z = mean_squared_error(yn, p50n) ** 0.5\n",
        "    cov80  = float(np.mean((yn >= p10n) & (yn <= p90n))) if len(yn) else np.nan\n",
        "\n",
        "    return mae_z, rmse_z, cov80, yn, p50n, p10n, p90n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "- Trains for **5 epochs** with AdamW: forward → clamp preds in z-space → `safe_pinball_loss` → backprop with grad clipping (1.0) → step.\n",
        "- After each epoch, runs **normalized eval** on train/valid (reports z-MAE, z-RMSE, 80% coverage) and prints a **debug snapshot** of batch quantiles (P10/P50/P90 percentiles) to monitor calibration.\n"
      ],
      "metadata": {
        "id": "QxbA9sy40W5q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCCu8KtvXNIH",
        "outputId": "58068804-6736-4485-9ccf-cfd06a2df33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=0.0502 | train z-MAE=0.0922 z-RMSE=0.3291 COV80=0.92 || valid z-MAE=0.0891 z-RMSE=0.3182 COV80=0.94\n",
            "pred z-quantiles (batch sample) P10/P50/P90: [-1.55677402  0.43040392  0.51835746] [-1.4777895   0.55597201  0.65233166] [-1.39874369  0.65306416  0.7579147 ]\n",
            "Epoch 2: loss=0.0415 | train z-MAE=0.0887 z-RMSE=0.3155 COV80=0.84 || valid z-MAE=0.0895 z-RMSE=0.2922 COV80=0.84\n",
            "pred z-quantiles (batch sample) P10/P50/P90: [-1.58486725  0.53067109  0.64163725] [-1.53040472  0.60428077  0.72626367] [-1.43626112  0.67472291  0.77879852]\n",
            "Epoch 3: loss=0.0406 | train z-MAE=0.0952 z-RMSE=0.3277 COV80=0.80 || valid z-MAE=0.0963 z-RMSE=0.3218 COV80=0.84\n",
            "pred z-quantiles (batch sample) P10/P50/P90: [-1.61726528  0.50416547  0.63571354] [-1.5847495   0.58893105  0.70747436] [-1.50379894  0.64438543  0.75570304]\n",
            "Epoch 4: loss=0.0396 | train z-MAE=0.0993 z-RMSE=0.3225 COV80=0.53 || valid z-MAE=0.1001 z-RMSE=0.3085 COV80=0.59\n",
            "pred z-quantiles (batch sample) P10/P50/P90: [-1.60767783  0.47705965  0.67070086] [-1.5597059   0.55055243  0.7265889 ] [-1.48581878  0.60375729  0.76225051]\n",
            "Epoch 5: loss=0.0357 | train z-MAE=0.0866 z-RMSE=0.3229 COV80=0.87 || valid z-MAE=0.0904 z-RMSE=0.3098 COV80=0.82\n",
            "pred z-quantiles (batch sample) P10/P50/P90: [-1.5027264   0.55090865  0.65864558] [-1.45862162  0.61859721  0.72556765] [-1.39836221  0.6727576   0.77865146]\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "for e in range(1, EPOCHS+1):\n",
        "    ts_model.train()\n",
        "    total = 0.0\n",
        "    for b in train_loader:\n",
        "        optim.zero_grad()\n",
        "        out = ts_model(b[\"val_norm\"].to(device),\n",
        "                       b[\"X_num\"].to(device),\n",
        "                       b[\"X_cat\"].to(device))\n",
        "        out = torch.clamp(out, -10.0, 10.0)\n",
        "        loss = safe_pinball_loss(out, b[\"y_next\"].to(device))\n",
        "        if not torch.isfinite(loss):\n",
        "            continue\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(ts_model.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        total += float(loss.item())\n",
        "\n",
        "    tr = evaluate_normalized(ts_model, train_loader, device)\n",
        "    va = evaluate_normalized(ts_model, valid_loader, device)\n",
        "    print(f\"Epoch {e}: loss={total/max(len(train_loader),1):.4f} | \"\n",
        "          f\"train z-MAE={tr[0]:.4f} z-RMSE={tr[1]:.4f} COV80={tr[2]:.2f} || \"\n",
        "          f\"valid z-MAE={va[0]:.4f} z-RMSE={va[1]:.4f} COV80={va[2]:.2f}\")\n",
        "    with torch.no_grad():\n",
        "      dbg = out.detach().cpu().numpy().reshape(-1,3)  # from a batch\n",
        "      print(\"pred z-quantiles (batch sample) P10/P50/P90:\",\n",
        "            np.percentile(dbg[:,0], [10,50,90]),\n",
        "            np.percentile(dbg[:,1], [10,50,90]),\n",
        "            np.percentile(dbg[:,2], [10,50,90]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse, cov, y, p50, p10, p90 = evaluate(ts_model, valid_loader, y_mean, y_std, device)\n",
        "print(mae, rmse, cov)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPUXqb3SitYi",
        "outputId": "ae66a6cc-3dee-4b74-8936-1534dc847b04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "739264.5954921641 1832695.283812338 0.8156565656565656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Seasonal naïve (t-7) baseline + 80% PI from train residuals\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_all = pd.concat([train_df, valid_df])\n",
        "y_all = df_all[TARGET].clip(lower=0)\n",
        "naive_pred = y_all.shift(7)\n",
        "\n",
        "# train residuals in log space\n",
        "train_mask = ~naive_pred.iloc[:len(train_df)].isna()\n",
        "r_train = np.log1p(y_all.iloc[:len(train_df)][train_mask]) - np.log1p(naive_pred.iloc[:len(train_df)][train_mask])\n",
        "q10, q90 = np.percentile(r_train, [10, 90])\n",
        "\n",
        "# valid metrics\n",
        "valid_idx = valid_df.index\n",
        "yp = naive_pred.loc[valid_idx].values\n",
        "ya = y_all.loc[valid_idx].values\n",
        "mask = np.isfinite(yp) & np.isfinite(ya)\n",
        "yp, ya = yp[mask], ya[mask]\n",
        "\n",
        "# build PI (apply residual quantiles to log-preds)\n",
        "logp = np.log1p(yp)\n",
        "p10 = np.expm1(logp + q10)\n",
        "p50 = yp\n",
        "p90 = np.expm1(logp + q90)\n",
        "\n",
        "mae = mean_absolute_error(ya, p50)\n",
        "rmse = mean_squared_error(ya, p50)**0.5\n",
        "cov80 = np.mean((ya >= p10) & (ya <= p90))\n",
        "\n",
        "print(f\"[Naive t-7] MAE={mae:,.0f}  RMSE={rmse:,.0f}  COV80={cov80:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPkHjwBL5fye",
        "outputId": "cf6ceb97-d8a7-4565-bbd1-4c99ba147295"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Naive t-7] MAE=1,160,776  RMSE=2,562,661  COV80=0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed linear baseline: scaled features, no target leakage, Ridge regularization\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# 1) Define numeric features without the target to avoid leakage\n",
        "NUMERIC_FEATS_BASE = [f for f in NUMERIC_FEATS if f != TARGET]\n",
        "\n",
        "# 2) Fit scaler (log1p mean/std) on TRAIN ONLY\n",
        "Xn_tr_raw = train_df[NUMERIC_FEATS_BASE].clip(lower=0).fillna(0).astype(float)\n",
        "mu = np.log1p(Xn_tr_raw).mean()\n",
        "sd = np.log1p(Xn_tr_raw).std().replace(0, 1)\n",
        "\n",
        "def build_X_scaled(df, mu, sd, train_cols=None):\n",
        "    Xn = df[NUMERIC_FEATS_BASE].clip(lower=0).fillna(0).astype(float)\n",
        "    Xn = (np.log1p(Xn) - mu) / (sd + 1e-8)\n",
        "    Xc = pd.get_dummies(df[CATEG_FEATS].astype(int), columns=CATEG_FEATS, drop_first=False)\n",
        "    X  = pd.concat([Xn, Xc], axis=1).fillna(0)\n",
        "    if train_cols is not None:\n",
        "        X = X.reindex(columns=train_cols, fill_value=0)\n",
        "    return X\n",
        "\n",
        "X_tr = build_X_scaled(train_df, mu, sd)\n",
        "train_cols = X_tr.columns\n",
        "X_va = build_X_scaled(valid_df, mu, sd, train_cols)\n",
        "\n",
        "# 3) Train Ridge on log1p(target)\n",
        "y_tr_log = np.log1p(train_df[TARGET].clip(lower=0).values)\n",
        "ridge = Ridge(alpha=1.0, random_state=0).fit(X_tr, y_tr_log)\n",
        "\n",
        "# 4) Predictions + 80% PI from train residuals (in log space)\n",
        "pred_va_log = ridge.predict(X_va)\n",
        "p50 = np.expm1(pred_va_log)\n",
        "\n",
        "r_train = y_tr_log - ridge.predict(X_tr)\n",
        "q10, q90 = np.percentile(r_train, [10, 90])\n",
        "p10 = np.expm1(pred_va_log + q10)\n",
        "p90 = np.expm1(pred_va_log + q90)\n",
        "\n",
        "y_va = valid_df[TARGET].clip(lower=0).values\n",
        "mae = mean_absolute_error(y_va, p50)\n",
        "rmse = mean_squared_error(y_va, p50) ** 0.5\n",
        "cov80 = np.mean((y_va >= p10) & (y_va <= p90))\n",
        "print(f\"[Ridge (fixed)] MAE={mae:,.0f}  RMSE={rmse:,.0f}  COV80={cov80:.2f}  (features={X_tr.shape[1]})\")\n",
        "\n",
        "# Optional: compute lift vs t-7 and vs TSLLM\n",
        "ts_mae, ts_rmse, ts_cov, *_ = evaluate(ts_model, valid_loader, y_mean, y_std, device)\n",
        "print(f\"Lift vs t-7  → TSLLM MAE: {(1 - 1_160_776/ts_mae):.1%} (use your latest t-7 numbers if different)\")\n",
        "print(f\"Lift vs Ridge→ TSLLM MAE: {(mae - ts_mae)/mae:.1%}, RMSE: {(rmse - ts_rmse)/rmse:.1%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C43xYEM16Nbp",
        "outputId": "5f027039-1818-47d4-d6a7-c7eec0b85b25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ridge (fixed)] MAE=1,312,100  RMSE=2,635,984  COV80=0.77  (features=67)\n",
            "Lift vs t-7  → TSLLM MAE: -57.0% (use your latest t-7 numbers if different)\n",
            "Lift vs Ridge→ TSLLM MAE: 43.7%, RMSE: 30.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3) Results & Baselines\n",
        "\n",
        "\n",
        "The LLM forecaster (TSLLM) materially outperforms reasonable baselines and is well-calibrated.\n",
        "- Accuracy: **MAE = 0.739M**, **RMSE = 1.832M**\n",
        "- Calibration: **COV80 = 0.816** (target ≈ 0.80)\n",
        "\n",
        "| Model            | MAE        | RMSE       | COV80 | Lift vs t−7 | Lift vs Ridge |\n",
        "|------------------|------------|------------|:-----:|------------:|--------------:|\n",
        "| **TSLLM (ours)** | **0.739M** | **1.832M** | 0.816 | —           | —             |\n",
        "| Seasonal t−7     | 1.161M     | 2.563M     | 0.75  | **−36.3% MAE**, **−28.5% RMSE** | — |\n",
        "| Ridge (fixed)    | 1.312M     | 2.636M     | 0.77  | —           | **−43.7% MAE**, **−30.5% RMSE** |\n",
        "\n",
        "*Lift = (baseline − model) / baseline.*\n",
        "\n",
        "**Interpretation**\n",
        "- TSLLM generalizes well (train ≈ valid in z-space) and provides **narrower errors** than both baselines.\n",
        "- **Coverage ~0.82** indicates slightly conservative intervals but close to the 80% target—acceptable for staffing use.\n",
        "\n"
      ],
      "metadata": {
        "id": "dQ_cUuh57WfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Observation and Next steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   The model demonstrates effective learning in its training space (normalized log-transformed ticket counts) with well-calibrated uncertainty estimates, achieving 36-44% accuracy improvements over seasonal naive and Ridge regression baselines. However, the current normalization strategy creates challenges for business interpretation, as the exponential denormalization amplifies small prediction errors significantly.\n",
        "2.   While the performance gains validate the technical approach, production deployment requires weighing these accuracy improvements against operational complexity.\n",
        "        *   The LLM-based system requires GPU infrastructure, complex retraining procedures, and significant computational overhead compared to simpler baselines that achieve reasonable performance with minimal operational burden.\n",
        "        *   In many production environments, the interpretability, operational simplicity, and instant retrainability of traditional forecasting methods may outweigh the accuracy benefits of the transformer approach.\n",
        "\n",
        "3.   For production deployment, I recommend either direct log-space prediction to address the interpretability issues, or a cost-benefit analysis comparing the operational overhead of the LLM system against the business value of the accuracy improvements. The architecture successfully demonstrates that large language models can be adapted for time series forecasting, but practical deployment depends on whether the specific use case justifies the complexity.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "v__ppJM1RSBy"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}